--- ./orig/MPI_File_write_ordered.c	2019-02-19 13:52:58.000000001 -0500
+++ ./src/MPI_File_write_ordered.c	2019-01-08 11:09:49.000000001 -0500
@@ -1,3 +1,61 @@
+/* 
+MPI_File_write_ordered
+
+   Collective write using shared file pointer
+int MPI_File_write_ordered(
+  MPI_File mpi_fh,
+  void *buf,
+  int count,
+  MPI_Datatype datatype,
+  MPI_Status *status
+);
+
+Parameters
+
+   mpi_fh
+          [in] file handle (handle)
+
+   buf
+          [in] initial address of buffer (choice)
+
+   count
+          [in] number of elements in buffer (nonnegative integer)
+
+   datatype
+          [in] datatype of each buffer element (handle)
+
+   status
+          [out] status object (Status)
+
+Remarks
+
+   MPI_FILE_WRITE_ORDERED is a collective version of the
+   MPI_FILE_WRITE_SHARED interface.
+
+   The semantics of a collective access using a shared file pointer is
+   that the accesses to the file will be in the order determined by the
+   ranks of the processes within the group. For each process, the location
+   in the file at which data is accessed is the position at which the
+   shared file pointer would be after all processes whose ranks within the
+   group less than that of this process had accessed their data. In
+   addition, in order to prevent subsequent shared offset accesses by the
+   same processes from interfering with this collective access, the call
+   might return only after all the processes within the group have
+   initiated their accesses. When the call returns, the shared file
+   pointer points to the next etype accessible, according to the file view
+   used by all processes, after the last etype requested.
+
+   Advice to users.
+
+   There may be some programs in which all processes in the group need to
+   access the file using the shared file pointer, but the program may not
+   require that data be accessed in order of process rank. In such
+   programs, using the shared ordered routines (e.g.,
+   MPI_FILE_WRITE_ORDERED rather than MPI_FILE_WRITE_SHARED) may enable an
+   implementation to optimize access, improving performance.
+
+*/
+ 
 // Copyright 2009 Deino Software. All rights reserved.
 // Source: http://mpi.deino.net/mpi_functions/index.htm
 
