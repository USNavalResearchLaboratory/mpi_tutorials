--- ./orig/MPI_Reduce.c	2019-02-19 13:52:58.000000001 -0500
+++ ./src/MPI_Reduce.c	2019-01-08 11:09:49.000000001 -0500
@@ -1,3 +1,143 @@
+/* 
+MPI_Reduce
+
+   Reduces values on all processes to a single value
+int MPI_Reduce(
+  void *sendbuf,
+  void *recvbuf,
+  int count,
+  MPI_Datatype datatype,
+  MPI_Op op,
+  int root,
+  MPI_Comm comm
+);
+
+Parameters
+
+   sendbuf
+          [in] address of send buffer (choice)
+
+   recvbuf
+          [out] address of receive buffer (choice, significant only at
+          root)
+
+   count
+          [in] number of elements in send buffer (integer)
+
+   datatype
+          [in] data type of elements of send buffer (handle)
+
+   op
+          [in] reduce operation (handle)
+
+   root
+          [in] rank of root process (integer)
+
+   comm
+          [in] communicator (handle)
+
+Remarks
+
+   MPI_REDUCE combines the elements provided in the input buffer of each
+   process in the group, using the operation op, and returns the combined
+   value in the output buffer of the process with rank root. The input
+   buffer is defined by the arguments sendbuf, count and datatype; the
+   output buffer is defined by the arguments recvbuf, count and datatype;
+   both have the same number of elements, with the same type. The routine
+   is called by all group members using the same arguments for count,
+   datatype, op, root and comm. Thus, all processes provide input buffers
+   and output buffers of the same length, with elements of the same type.
+   Each process can provide one element, or a sequence of elements, in
+   which case the combine operation is executed element-wise on each entry
+   of the sequence. For example, if the operation is MPI_MAX and the send
+   buffer contains two elements that are floating point numbers ( count =
+   2 and datatype = MPI_FLOAT), then [img123.gif] and [img124.gif] .
+
+   The operation op is always assumed to be associative. All predefined
+   operations are also assumed to be commutative. Users may define
+   operations that are assumed to be associative, but not commutative. The
+   "canonical" evaluation order of a reduction is determined by the ranks
+   of the processes in the group. However, the implementation can take
+   advantage of associativity, or associativity and commutativity in order
+   to change the order of evaluation. This may change the result of the
+   reduction for operations that are not strictly associative and
+   commutative, such as floating point addition.
+
+Thread and Interrupt Safety
+
+   This routine is thread-safe. This means that this routine may be safely
+   used by multiple threads without the need for any user-provided thread
+   locks. However, the routine is not interrupt safe. Typically, this is
+   due to the use of memory allocation routines such as malloc or other
+   non-MPICH runtime routines that are themselves not interrupt-safe.
+
+Notes for Fortran
+
+   All MPI routines in Fortran (except for MPI_WTIME and MPI_WTICK) have
+   an additional argument ierr at the end of the argument list. ierr is an
+   integer and has the same meaning as the return value of the routine in
+   C. In Fortran, MPI routines are subroutines, and are invoked with the
+   call statement.
+
+   All MPI objects (e.g., MPI_Datatype, MPI_Comm) are of type INTEGER in
+   Fortran.
+
+Notes on collective operations
+
+   The reduction functions (MPI_Op) do not return an error value. As a
+   result, if the functions detect an error, all they can do is either
+   call MPI_Abort or silently skip the problem. Thus, if you change the
+   error handler from MPI_ERRORS_ARE_FATAL to something else, for example,
+   MPI_ERRORS_RETURN, then no error may be indicated.
+
+   The reason for this is the performance problems in ensuring that all
+   collective routines return the same error value.
+
+Errors
+
+   All MPI routines (except MPI_Wtime and MPI_Wtick) return an error
+   value; C routines as the value of the function and Fortran routines in
+   the last argument. Before the value is returned, the current MPI error
+   handler is called. By default, this error handler aborts the MPI job.
+   The error handler may be changed with MPI_Comm_set_errhandler (for
+   communicators), MPI_File_set_errhandler (for files), and
+   MPI_Win_set_errhandler (for RMA windows). The MPI-1 routine
+   MPI_Errhandler_set may be used but its use is deprecated. The
+   predefined error handler MPI_ERRORS_RETURN may be used to cause error
+   values to be returned. Note that MPI does not guarentee that an MPI
+   program can continue past an error; however, MPI implementations will
+   attempt to continue whenever possible.
+
+   MPI_SUCCESS
+          No error; MPI routine completed successfully.
+
+   MPI_ERR_COMM
+          Invalid communicator. A common error is to use a null
+          communicator in a call (not even allowed in MPI_Comm_rank).
+
+   MPI_ERR_COUNT
+          Invalid count argument. Count arguments must be non-negative; a
+          count of zero is often valid.
+
+   MPI_ERR_TYPE
+          Invalid datatype argument. May be an uncommitted MPI_Datatype
+          (see MPI_Type_commit).
+
+   MPI_ERR_BUFFER
+          Invalid buffer pointer. Usually a null buffer where one is not
+          valid.
+
+   MPI_ERR_BUFFER
+          This error class is associcated with an error code that
+          indicates that two buffer arguments are aliased; that is, the
+          describe overlapping storage (often the exact same storage).
+          This is prohibited in MPI (because it is prohibited by the
+          Fortran standard, and rather than have a separate case for C and
+          Fortran, the MPI Forum adopted the more restrictive requirements
+          of Fortran).
+
+*/
+ 
 // Copyright 2009 Deino Software. All rights reserved.
 // Source: http://mpi.deino.net/mpi_functions/index.htm
 
